{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi Select Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "<a href=\"https://colab.research.google.com/github/Michel-p16/PDS-Project/blob/capstone_korbi/distilbert_multi_trainingwQ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "#connect drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Pfad zum neuen Multi-Select-Dataset\n",
    "dataset_path_multi = '/content/drive/My Drive/ColabData/final_multi_question_data.json'\n",
    "\n",
    "# Laden des neuen Datensatzes\n",
    "import json\n",
    "with open(dataset_path_multi, 'r') as file:\n",
    "    dataset_multi = json.load(file)\n",
    "\n",
    "!pip install transformers datasets\n",
    "#1. Daten laden + filtern (MULTI SELECT here)\n",
    "import json\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Filtere das Dataset nach Fragen mit dem Typ \"MULTI_SELECT\"\n",
    "filtered_dataset_multi = [example for example in dataset_multi if example[\"type\"] == \"MULTI_SELECT\"]\n",
    "\n",
    "\n",
    "\n",
    "#2. Daten formatieren (diesmal inkl Fragen Einbezug für training) -> binäre vektoren durch MultiLabelBinarizer\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# Labels direkt in Float32 konvertieren\n",
    "def convert_to_multi_select_format(data, mlb):\n",
    "    formatted_data = []\n",
    "\n",
    "    for example in data:\n",
    "        question = example[\"question\"]\n",
    "        answers = example[\"answers\"]\n",
    "\n",
    "        for answer in answers:\n",
    "            text = answer.get(\"answer_text\", \"\")\n",
    "            labels = answer.get(\"answer_label\", \"\").split(\",\")\n",
    "\n",
    "            # Labels bereinigen\n",
    "            labels = [label.strip() for label in labels]\n",
    "\n",
    "            if labels:  # Falls Labels vorhanden sind\n",
    "                label_vector = mlb.transform([labels])[0]  # Binärvektor\n",
    "                label_vector = label_vector.astype(float)  # **Hier direkt in float konvertieren!**\n",
    "\n",
    "                formatted_data.append({\n",
    "                    \"question\": question,  # Frage bleibt erhalten\n",
    "                    \"text\": text,  # Antwort bleibt erhalten\n",
    "                    \"labels\": label_vector  # Labels nun als float\n",
    "                })\n",
    "\n",
    "    return formatted_data\n",
    "\n",
    "\n",
    "\n",
    "# Alle eindeutigen Labels aus dem Datensatz sammeln\n",
    "all_labels = set()\n",
    "for example in filtered_dataset_multi:\n",
    "    for answer in example[\"answers\"]:\n",
    "        labels = answer.get(\"answer_label\", \"\").split(\",\")\n",
    "        labels = [label.strip() for label in labels]  # Führende/nachfolgende Leerzeichen entfernen\n",
    "        all_labels.update(labels)\n",
    "\n",
    "# MultiLabelBinarizer initialisieren\n",
    "multi_label_binarizer = MultiLabelBinarizer(classes=sorted(list(all_labels)))  # Sortiert für Konsistenz\n",
    "multi_label_binarizer.fit([list(all_labels)])\n",
    "\n",
    "# Daten formatieren\n",
    "formatted_multi_dataset = convert_to_multi_select_format(filtered_dataset_multi, multi_label_binarizer)\n",
    "\n",
    "# Überprüfung der Ergebnisse\n",
    "print(f\"Anzahl der formatierten Beispiele: {len(formatted_multi_dataset)}\")\n",
    "if formatted_multi_dataset:\n",
    "    print(f\"Beispiel: {formatted_multi_dataset[0]}\")\n",
    "print(f\"Alle möglichen Labels: {multi_label_binarizer.classes_}\")\n",
    "\n",
    "#3. Daten splitten + formatieren in Hugging Face Dataset\n",
    "\n",
    "#Split 80/20\n",
    "train_data_multi_formatted, eval_data_multi_formatted = train_test_split(formatted_multi_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Multi-Label-Trainings- und Evaluations-Dataset erstellen\n",
    "train_dataset_multi = Dataset.from_pandas(pd.DataFrame(train_data_multi_formatted))\n",
    "eval_dataset_multi = Dataset.from_pandas(pd.DataFrame(eval_data_multi_formatted))\n",
    "\n",
    "# Überprüfung der Datenmengen\n",
    "print(f\"Anzahl der Trainingsdaten: {len(train_dataset_multi)}\")\n",
    "print(f\"Anzahl der Evaluationsdaten: {len(eval_dataset_multi)}\")\n",
    "#4. Dataset vorverarbeitung\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "\n",
    "# Tokenizer initialisieren\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Vorverarbeitungsfunktion für Multi-Label-Daten\n",
    "def preprocess_function_multi(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"question\"],  # Frage\n",
    "        examples[\"text\"],      # Antwort\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128  # Falls nötig, auf 128 erhöht\n",
    "    )\n",
    "\n",
    "# Tokenizer-Funktion an\n",
    "train_dataset_multi = train_dataset_multi.map(preprocess_function_multi, batched=True)\n",
    "eval_dataset_multi = eval_dataset_multi.map(preprocess_function_multi, batched=True)\n",
    "\n",
    "# PyTorch-kompatibles Format setzen\n",
    "train_dataset_multi.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataset_multi.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Struktur überprüfen\n",
    "print(train_dataset_multi[0])\n",
    "print(train_dataset_multi[0][\"labels\"])  # Sollte ein Float-Tensor sein\n",
    "print(train_dataset_multi[0][\"labels\"].dtype)  # Erwartet: torch.float32\n",
    "\n",
    "\n",
    "#5. PyTorch-kompatibles Format setzen\n",
    "train_dataset_multi.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "eval_dataset_multi.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "#6. Training\n",
    "\n",
    "import pickle\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from transformers import DistilBertForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "# Loss-Funktion für Multi-Label-Klassifikation\n",
    "def custom_loss(predictions, labels):\n",
    "    return F.binary_cross_entropy_with_logits(predictions, labels)\n",
    "\n",
    "# Eigene Trainer-Klasse mit angepasster Loss-Funktion\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")  # Entferne Labels aus Inputs\n",
    "        outputs = model(**inputs)      # Modell-Vorhersagen\n",
    "        logits = outputs.logits        # Logits extrahieren\n",
    "        loss = custom_loss(logits, labels)  # BCEWithLogitsLoss berechnen\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "# Modell initialisieren (Multi-Label-Modus)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\",\n",
    "    num_labels=len(multi_label_binarizer.classes_),  # Anzahl der Labels\n",
    "    problem_type=\"multi_label_classification\"  # WICHTIG für Multi-Label\n",
    ")\n",
    "\n",
    "# Speichern des Label-Mappings für späteres Decoding\n",
    "label_mapping = {idx: label for idx, label in enumerate(multi_label_binarizer.classes_)}\n",
    "inverse_label_mapping = {label: idx for idx, label in enumerate(multi_label_binarizer.classes_)}\n",
    "\n",
    "label_mapping_path = \"/content/drive/My Drive/label_mapping_multi_wQ.pkl\"\n",
    "with open(label_mapping_path, \"wb\") as file:\n",
    "    pickle.dump(label_mapping, file)\n",
    "\n",
    "print(\"Label-Mapping erfolgreich gespeichert.\")\n",
    "\n",
    "# Anpassung der Metriken (für Multi-Label Klassifikation)\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids  # Wahre Labels\n",
    "    preds = torch.sigmoid(torch.tensor(pred.predictions)) > 0.5  # Wahrscheinlichkeiten -> Binärwerte\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds.numpy(), average=\"weighted\"\n",
    "    )\n",
    "    acc = (preds.numpy() == labels).all(axis=1).mean()  # Beispielgenauigkeit\n",
    "    return {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }\n",
    "\n",
    "# TrainingArgs\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=20,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,  # Regularisierung -> erhöhen danach .05\n",
    "    #adam_beta1=0.9, #senken falls langsam reaktion\n",
    "    #adam_beta2=0.999, #senekn bei wenig lernprogress\n",
    "    #adam_epsilon=1e-6,  # erhöhen falls instabil\n",
    "    #label_smoothing_factor=0.1,  #bessere Generalisierung\n",
    "    evaluation_strategy=\"epoch\",  # eval pro epoch\n",
    "    save_strategy=\"epoch\",  # save pro epoch\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,  # save best\n",
    "    metric_for_best_model=\"f1\",  # opt F1\n",
    "    greater_is_better=True,  # higher better F1\n",
    "    report_to=\"none\",  # ausgeschaltet, weil kein Zugriff zu repo\n",
    "    logging_steps=10,  # Alle 10 Schritte loggen\n",
    ")\n",
    "\n",
    "# Trainer-Objekt erstellen\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset_multi,\n",
    "    eval_dataset=eval_dataset_multi,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Training starten\n",
    "trainer.train()\n",
    "\n",
    "# Modell speichern\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "model.save_pretrained(\"/content/drive/My Drive/multi_distilbert_wQ\")\n",
    "tokenizer.save_pretrained(\"/content/drive/My Drive/multi_distilbert_wQ\")\n",
    "\n",
    "print(\"Das Modell wurde erfolgreich gespeichert.\")\n",
    "\n",
    "\n",
    "#7. manueller test\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Modell & Tokenizer laden\n",
    "model_path = \"/content/drive/My Drive/multi_distilbert_wQ\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Label Mapping laden\n",
    "label_mapping_path = \"/content/drive/My Drive/label_mapping_multi_wQ.pkl\"\n",
    "with open(label_mapping_path, \"rb\") as file:\n",
    "    label_mapping = pickle.load(file)\n",
    "\n",
    "# Fragen aus dem Dataset extrahieren\n",
    "all_questions = list(set(example[\"question\"] for example in dataset_multi))\n",
    "\n",
    "# 5 Zufällige Frage auswählen\n",
    "selected_questions = random.sample(all_questions, 5)\n",
    "\n",
    "# Evaluation für jede Frage\n",
    "for idx, question in enumerate(selected_questions, start=1):\n",
    "    print(f\"Frage {idx}: {question}\")\n",
    "\n",
    "    # Manuelle Eingabe der Antwort\n",
    "    user_answer = input(\"Bitte geben Sie eine Antwort ein: \")\n",
    "\n",
    "    # Eingabe tokenisieren (Frage + Antwort)\n",
    "    inputs = tokenizer(\n",
    "        question,  # Frage\n",
    "        user_answer,  # Antwort\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "    # Modellvorhersage (ohne Gradientenberechnung)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits  # Rohwerte des Modells\n",
    "\n",
    "    # Wahrscheinlichkeiten berechnen\n",
    "    probs = torch.sigmoid(logits).squeeze().tolist()\n",
    "\n",
    "    # Schwellenwert setzen (z.B. 0.5)\n",
    "    threshold = 0.25\n",
    "    predicted_labels_indices = [idx for idx, prob in enumerate(probs) if prob > threshold]\n",
    "\n",
    "    # Vorhergesagte Labels mappen\n",
    "    predicted_labels = [label_mapping[idx] for idx in predicted_labels_indices]\n",
    "\n",
    "    # Ausgabe der Vorhersagen\n",
    "    print(\"Predicted weighted vector (probabilities for each label):\")\n",
    "    print(probs)\n",
    "\n",
    "    print(\"Predicted binary vector (thresholded):\")\n",
    "    binary_vector = [1 if prob > threshold else 0 for prob in probs]\n",
    "    print(binary_vector)\n",
    "\n",
    "    print(\"Predicted Labels:\")\n",
    "    print(predicted_labels if predicted_labels else \"Keine Labels vorhergesagt\")\n",
    "\n",
    "    print(\"-\" * 60)  # Trennlinie für bessere Übersicht\n",
    "\n",
    "!pip install tensorboard\n",
    "# TensorBoard-Extension laden\n",
    "%load_ext tensorboard\n",
    "\n",
    "# TensorBoard starten und Logs visualisieren\n",
    "%tensorboard --logdir ./logs\n",
    "\n",
    "import os\n",
    "\n",
    "log_dir = \"./logs\"\n",
    "print(\"Inhalt des Log-Verzeichnisses:\", os.listdir(log_dir))\n",
    "\n",
    "import os\n",
    "\n",
    "fit_dir = \"./logs/fit\"\n",
    "print(\"Inhalt des fit-Ordners:\", os.listdir(fit_dir))\n",
    "\n",
    "fit_subdir = \"./logs/fit/20250130-114034\"\n",
    "print(\"Inhalt des Unterordners:\", os.listdir(fit_subdir))\n",
    "\n",
    "\n",
    "train_log_dir = \"./logs/fit/20250130-114034/train\"\n",
    "print(\"Inhalt des train-Ordners:\", os.listdir(train_log_dir))\n",
    "\n",
    "\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./logs/fit/20250130-114034/train\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
